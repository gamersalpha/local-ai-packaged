# ğŸ§  Local AI Packaged â€” Installation et utilisation

**Local AI Packaged** est une stack toutâ€‘enâ€‘un pour hÃ©berger un environnement complet dâ€™IA locale :  
Ollama (LLMs), Open WebUI, Flowise, n8n, Supabase, Langfuse, Neo4j, Qdrant, et plus encore â€” le tout orchestrÃ© via Docker Compose.

Ce projet est conÃ§u pour les **autoâ€‘hÃ©bergeurs techniques** souhaitant dÃ©ployer leur propre infrastructure IA sur un serveur local (NAS, miniâ€‘PC, ou machine dÃ©diÃ©e).

---

## âš™ï¸ 1. PrÃ©requis

Avant de commencer, assurezâ€‘vous dâ€™avoir :

- ğŸ³ **Docker** et **Docker Compose** installÃ©s  
- ğŸ **Python 3.8+**  
- ğŸ’¾ **Git**  
- ğŸ’¡ Environ **16â€¯Go de RAM** minimum recommandÃ©s

### GPU (optionnel)

- **NVIDIA :** [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)  
- **AMD :** ROCm configurÃ©  
- **CPU uniquement :** fonctionne aussi, plus lentement

---

## ğŸš€ 2. Installation rapide

### â‘  Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/gamersalpha/local-ai-packaged.git
cd local-ai-packaged
```

### â‘¡ GÃ©nÃ©rer le fichier `.env` automatiquement

Le script `generate_env.py` crÃ©e automatiquement toutes les variables nÃ©cessairesâ€¯:  
- mots de passe et clÃ©s alÃ©atoires  
- dÃ©tection du GPU (NVIDIA / AMD / CPU)  
- chemins locaux corrects

```bash
python3 generate_env.py --yes --regen-sensitive --docker
```

Une fois terminÃ©, le script affiche les URLs locales de vos servicesâ€¯:

```
ğŸŒ Services disponibles :
  ğŸ§  Ollama      : http://192.168.1.42:11434
  âš™ï¸ n8n          : http://192.168.1.42:5678
  ğŸ’¬ Open WebUI  : http://192.168.1.42:8080
  ğŸ§± Supabase    : http://192.168.1.42:54323
```

---

## ğŸ§© 3. Lancer et gÃ©rer les services

### â–¶ï¸ DÃ©marrage automatisÃ©

Le script `start_services.py` orchestre tout le dÃ©ploiement :  
- vÃ©rifie ou crÃ©e le `.env`  
- clone automatiquement la stack Docker de **Supabase**  
- (dÃ©s)active Caddy si besoin  
- gÃ©nÃ¨re la clÃ© SearXNG  
- dÃ©marre tous les conteneurs dans le bon ordre

```bash
python3 start_services.py --profile cpu
```

#### Options disponibles

| Option | Description |
|--------|--------------|
| `--profile [cpu|gpu-nvidia|gpu-amd]` | Choix du profil matÃ©riel |
| `--environment [private|public]` | Mode de dÃ©ploiement |
| `--no-supabase` | DÃ©sactive Supabase |
| `--no-caddy` | DÃ©sactive Caddy |
| `--update` | Tire les derniÃ¨res images Docker |
| `--dry-run` | Simule le lancement sans exÃ©cuter Docker |

Exemplesâ€¯:

```bash
# Lancement CPU
python3 start_services.py --profile cpu

# Lancement GPU (NVIDIA)
python3 start_services.py --profile gpu-nvidia

# Mise Ã  jour avant lancement
python3 start_services.py --update
```

---

### â™»ï¸ Mettre Ã  jour la stack

Utilisez le script `update_services.sh` pour tout rafraÃ®chir automatiquementâ€¯:

```bash
./update_services.sh
```

Ce scriptâ€¯:
1. stoppe tous les conteneurs  
2. tire les nouvelles images Docker  
3. redÃ©marre la stack via `start_services.py`

---

## ğŸŒ 4. AccÃ¨s aux interfaces

| Service | Description | URL locale par dÃ©faut |
|----------|--------------|-----------------------|
| **n8n** | Workflow automation | http://192.168.1.42:5678 |
| **Open WebUI** | Interface de chat LLM | http://192.168.1.42:8080 |
| **Ollama** | API locale des modÃ¨les | http://192.168.1.42:11434 |
| **Flowise** | Concepteur de pipelines IA | http://192.168.1.42:3001 |
| **Supabase** | Base de donnÃ©es et auth | http://192.168.1.42:54323 |
| **Langfuse** | Suivi des appels LLM | http://192.168.1.42:3000 |
| **SearXNG** | Recherche web RAG | http://192.168.1.42:8080 |
| **Neo4j** | Base graphe | http://192.168.1.42:7474 |

*(Adaptez les IP selon votre rÃ©seau local)*

---

## ğŸ› ï¸ 5. DÃ©pannage rapide

### Supabase ne dÃ©marre pasâ€¯?
- VÃ©rifiez que le dossier `supabase/` a bien Ã©tÃ© crÃ©Ã© automatiquement.  
- Supprimezâ€‘le et relancezâ€¯: `python3 start_services.py --no-caddy`  
- Si besoin, ajoutez dans `.env` : `POOLER_DB_POOL_SIZE=5`

### GPU non dÃ©tectÃ©â€¯?
- VÃ©rifiez que le **NVIDIA Container Toolkit** ou **ROCm** est bien installÃ©.  
- Vous pouvez toujours dÃ©marrer en mode CPUâ€¯:  
  ```bash
  python3 start_services.py --profile cpu
  ```

### Ports dÃ©jÃ  utilisÃ©sâ€¯?
Modifiez les ports directement dans `docker-compose.yml` avant de relancer.

---

## ğŸ§¾ Structure du projet

```
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ start_services.py
â”œâ”€â”€ update_services.sh
â”œâ”€â”€ generate_env.py
â”œâ”€â”€ supabase/           # auto-clonÃ© par start_services.py (ignorÃ© par Git)
â”œâ”€â”€ n8n/
â”‚   â””â”€â”€ backup/
â”œâ”€â”€ searxng/
â”œâ”€â”€ shared/
â””â”€â”€ neo4j/
```

> ğŸ“˜ Le dossier `supabase/` est gÃ©nÃ©rÃ© automatiquement et **ne doit pas Ãªtre versionnÃ©**.  
> Il est dÃ©jÃ  listÃ© dans `.gitignore`.

---

## ğŸ“œ Licence

Sous licence **ApacheÂ 2.0**.  
Voir [LICENSE](LICENSE) pour plus dâ€™informations.

---

**CrÃ©Ã© avec â¤ï¸ pour la communautÃ© autoâ€‘hÃ©bergeuse.**
