# ğŸ§  Local AI Packaged

> âš ï¸ **Note:** This project is a maintained and extended **fork** of [coleam00/local-ai-packaged](https://github.com/coleam00/local-ai-packaged), which no longer appears to be actively maintained.  
> This version includes upâ€‘toâ€‘date dependencies, bug fixes, and full automation scripts for setup and management.

---

## ğŸŒ Overview

**Local AI Packaged** provides a full selfâ€‘hosted AI environment using **Docker Compose**.  
It bundles everything you need for local LLM workflows, including:

- ğŸ§  **Ollama** for local LLM inference  
- ğŸ’¬ **Open WebUI** for chatting with your models or n8n agents  
- âš™ï¸ **n8n** for building automation workflows  
- ğŸ§± **Supabase** as database, vector store, and auth system  
- ğŸ§© **Flowise**, **Langfuse**, **Qdrant**, **Neo4j**, **SearXNG**, and optional **Caddy**

This version is designed for **technical selfâ€‘hosters** running the stack on a home server, NAS (e.g. Synology), or any Linux host.

---

## âš™ï¸ Prerequisites

Make sure you have:

- ğŸ³ **Docker** and **Docker Compose** installed  
- ğŸ **Python 3.8+**  
- ğŸ’¾ **Git**  
- ğŸ’¡ At least **16â€¯GB RAM** recommended

### GPU (optional)

- **NVIDIA:** [Install NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)  
- **AMD:** ROCm runtime configured  
- **CPU only:** Works fine, just slower

---

## ğŸš€ Quick Installation

### â‘  Clone the repository

```bash
git clone https://github.com/gamersalpha/local-ai-packaged.git
cd local-ai-packaged
```

### â‘¡ Generate the `.env` automatically

The `generate_env.py` script creates a full environment configuration automatically:  
- Generates secure random secrets  
- Detects GPU type (NVIDIA / AMD / CPU)  
- Sets correct local paths for volumes  

Run:

```bash
python3 generate_env.py --yes --regen-sensitive --docker
```

Once complete, it will start the stack and show the service URLs:

```
ğŸŒ Available services:
  ğŸ§  Ollama      : http://192.168.1.42:11434
  âš™ï¸ n8n          : http://192.168.1.42:5678
  ğŸ’¬ Open WebUI  : http://192.168.1.42:8080
  ğŸ§± Supabase    : http://192.168.1.42:54323
```

---

## ğŸ§© Managing the stack

### â–¶ï¸ Start automatically

Use the provided Python launcher for easy orchestration:

```bash
python3 start_services.py [options]
```

#### Options

| Option | Description |
|--------|--------------|
| `--profile [cpu|gpu-nvidia|gpu-amd]` | Select hardware profile |
| `--environment [private|public]` | Deployment environment |
| `--no-supabase` | Skip Supabase include |
| `--no-caddy` | Skip Caddy reverse proxy |
| `--update` | Pull latest Docker images before start |
| `--dry-run` | Preview configuration only |

#### Examples

```bash
# CPU-only deployment
python3 start_services.py --profile cpu

# NVIDIA GPU
python3 start_services.py --profile gpu-nvidia

# Skip Supabase and Caddy
python3 start_services.py --no-supabase --no-caddy

# Pull new images before starting
python3 start_services.py --update
```

ğŸ’¡ **What it does:**
- Validates or creates your `.env`
- Clones Supabaseâ€™s official Docker stack if missing
- (De)comments Caddy and Supabase dynamically
- Generates a new secret key for SearXNG
- Stops existing containers before redeploy
- Starts Supabase â†’ then the Local AI stack

---

### â™»ï¸ Update all services

Use the helper script `update_services.sh`:

```bash
./update_services.sh
```

This will:
1. Stop all containers  
2. Pull the latest images  
3. Restart the stack via `start_services.py`

For GPU users, you can adjust:
```bash
docker compose -p localai -f docker-compose.yml --profile gpu-nvidia down
docker compose -p localai -f docker-compose.yml --profile gpu-nvidia pull
python3 start_services.py --profile gpu-nvidia --no-caddy
```

---

## ğŸŒ Access your local services

| Service | Description | Default URL |
|----------|--------------|-------------|
| **n8n** | Workflow automation | http://192.168.1.42:5678 |
| **Open WebUI** | Chat interface for LLMs | http://192.168.1.42:8080 |
| **Ollama** | Local LLM API | http://192.168.1.42:11434 |
| **Flowise** | Lowâ€‘code AI builder | http://192.168.1.42:3001 |
| **Supabase** | DB, Auth & Vector store | http://192.168.1.42:54323 |
| **Langfuse** | LLM tracing dashboard | http://192.168.1.42:3000 |
| **SearXNG** | Web search engine for RAG | http://192.168.1.42:8080 |
| **Neo4j** | Graph database | http://192.168.1.42:7474 |

*(Adjust IPs for your own LAN setup.)*

---

## ğŸ› ï¸ Troubleshooting

### Supabase fails to start
- Check that the folder `supabase/` was created automatically.  
- Delete it and rerun:  
  ```bash
  python3 start_services.py --no-caddy
  ```
- Ensure `.env` contains:  
  ```bash
  POOLER_DB_POOL_SIZE=5
  ```

### GPU not detected
- Ensure the **NVIDIA Container Toolkit** or **ROCm** is installed correctly.  
- Fallback to CPU mode if needed:  
  ```bash
  python3 start_services.py --profile cpu
  ```

### Ports already in use
Edit `docker-compose.yml` to change exposed ports, then restart.

---

## ğŸ§¾ Project structure

```
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ start_services.py
â”œâ”€â”€ update_services.sh
â”œâ”€â”€ generate_env.py
â”œâ”€â”€ supabase/           # auto-cloned (ignored by Git)
â”œâ”€â”€ n8n/
â”‚   â””â”€â”€ backup/
â”œâ”€â”€ searxng/
â”œâ”€â”€ shared/
â””â”€â”€ neo4j/
```

> ğŸ“ **Note:** The `supabase/` folder is automatically cloned by `start_services.py` from [github.com/supabase/supabase](https://github.com/supabase/supabase).  
> It should **not** be committed to Git and is already included in `.gitignore`.

---

## ğŸ“œ License

Licensed under the **Apache 2.0 License**.  
See [LICENSE](LICENSE) for details.

---

**Built and maintained with â¤ï¸ for the selfâ€‘hosting community.**
